# Igbo Archives Scrapers

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

This repository contains the Python scraper scripts for the Igbo Archives data collection project. These scripts are designed to be modular, robust, and maintainable, with each script targeting a specific cultural or historical source.

This repository contains **only the scraper code**. The data generated by these scripts is stored in dedicated, separate datasets on Hugging Face.

## üèõÔ∏è Project Philosophy: Code vs. Data

This project strictly separates the **code** (which you own) from the **data** (which you are archiving).

* **CODE (This Repo):**
    * **Location:** `github.com/Nwokike/igbo-archives-scrapers`
    * **Contents:** All `.py` scraper scripts.
    * **License:** **Apache 2.0**. This gives others permission to use, modify, and build upon *your code*.

* **DATA (Hugging Face):**
    * **Location:** `huggingface.co/datasets/nwokikeonyeka/...`
    * **Contents:** The `.jsonl` data and all scraped `/images`.
    * **License:** **`other`**. The license for the *data* belongs to the original sources (e.g., MAA Cambridge, G.I. Jones Estate). Each dataset has a specific `README.md` explaining the copyright.

## üìä Data Sources

Each script in the `/scrapers` directory is self-contained and pushes to its own Hugging Face dataset.

| Source | Scraper Script | Data Type | Hugging Face Dataset |
| :--- | :--- | :--- | :--- |
| **Ukpuru Blog** | `run_ukpuru.py` | Secondary | [`ukpuru_blog_dataset`](https://huggingface.co/datasets/nwokikeonyeka/ukpuru_blog_dataset) |
| **G.I. Jones Archive** | `run_gijones.py` | Primary | [`gi_jones_archive_dataset`](https://huggingface.co/datasets/nwokikeonyeka/gi_jones_archive_dataset) |
| *(More coming soon)* | | | |

## üöÄ Getting Started

1.  **Clone this repository:**
    ```bash
    git clone [https://github.com/Nwokike/igbo-archives-scrapers.git](https://github.com/Nwokike/igbo-archives-scrapers.git)
    cd igbo-archives-scrapers
    ```

2.  **Install dependencies:**
    (You must have Python 3 installed)
    ```bash
    pip install -r requirements.txt
    ```

## ‚öôÔ∏è How to Run a Scraper

Each scraper is run individually from the command line.

1.  **Navigate to the `scrapers` directory:**
    ```bash
    cd scrapers
    ```

2.  **Run the desired script:**
    ```bash
    python run_gijones.py
    ```

3.  The script will prompt for your Hugging Face `WRITE` token.

4.  It will create local `data_raw` and `data_clean` folders, run the full scrape and clean process, and then automatically upload the clean data to its dedicated Hugging Face repository.

## ü§ù How to Add a New Source

This project is designed to be easily extended.

1.  Copy `scrapers/run_gijones.py` as a template for your new source (e.g., `run_maa_cambridge.py`).
2.  Update the **Configuration** variables at the top of the file (e.g., `REPO_ID`, `SOURCE_ID`, `BASE_URL`).
3.  Rewrite the `get_all_category_pages()` and `scrape_gallery_page()` functions to match the new site's HTML structure.
4.  Ensure the output `post_data` dictionary *strictly* follows the `schema_v1` (documented in the dataset cards).
5.  Run the script: `python run_maa_cambridge.py`.
6.  Submit a Pull Request.

## üìÑ License

The code in this repository is licensed under the [Apache 2.0 License](LICENSE).

This license **does not** apply to the data scraped by these scripts. The datasets on Hugging Face have their own `README.md` files that detail the specific copyright and usage terms for the content.
